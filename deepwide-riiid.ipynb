{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/riiid-test-answer-prediction/lectures.csv\n/kaggle/input/riiid-test-answer-prediction/example_sample_submission.csv\n/kaggle/input/riiid-test-answer-prediction/example_test.csv\n/kaggle/input/riiid-test-answer-prediction/questions.csv\n/kaggle/input/riiid-test-answer-prediction/train.csv\n/kaggle/input/riiid-test-answer-prediction/riiideducation/__init__.py\n/kaggle/input/riiid-test-answer-prediction/riiideducation/competition.cpython-37m-x86_64-linux-gnu.so\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow as tf\n# 数据地址\ndir_path = '/kaggle/input/riiid-test-answer-prediction/'\nfile_train = 'train.csv'\nfile_questions = 'questions.csv'","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 导入工具包\n# 数据大小\nnrows =  100 * 10000\n# nrows = None\n# 读取训练数据\ntrain = pd.read_csv(\n                    dir_path + file_train, \n                    nrows=nrows, \n                    usecols=['row_id', 'timestamp', 'user_id', 'content_id', \n                             'content_type_id', 'task_container_id', 'answered_correctly',\n                            'prior_question_elapsed_time','prior_question_had_explanation'],\n                    dtype={\n                            'row_id': 'int64',\n                            'timestamp': 'int64',\n                            'user_id': 'int32',\n                            'content_id': 'int16',\n                            'content_type_id': 'int8',\n                            'task_container_id': 'int8',\n                            'answered_correctly': 'int8',\n                            'prior_question_elapsed_time': 'float32',\n                            'prior_question_had_explanation': 'str'\n                        }\n                   )\n# 读取问题数据\nquestions = pd.read_csv(\n                        dir_path + file_questions, \n                        nrows=nrows,\n                        usecols=['question_id','bundle_id','part'], \n                        dtype={\n                           'question_id': 'int16',\n                           'bundle_id': 'int16',\n                           'part': 'int8',\n                       }\n                    )","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# prior_question_had_explanation 特征处理\ntrain['prior_question_had_explanation'] = train['prior_question_had_explanation'].map({'True':1,'False':0}).fillna(-1).astype(np.int8)\n# 只保留问题数据\ntrain = train[train['content_type_id']==0]\n# 释放内存\nimport gc\ngc.collect()\n# tailf选取部分数据进行建模，减少内存压力\nmax_num = 100\ntrain = train.groupby(['user_id']).tail(max_num)\n# 训练数据和问题数据进行关联\ntrain = pd.merge(\n        left=train,\n        right=questions,\n        how='left',\n        left_on='content_id',\n        right_on='question_id'\n        )\n# 训练数据填充0\ntrain = train.fillna(0)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 类别特征转换函数\nclass cat_deal:\n    def __init__(self):\n        self.max_len = 0\n        self.dict_map = {}\n    \n    def fit(self, cat_list):\n        index = 1 \n        for cat_i in cat_list:\n            if cat_i not in self.dict_map:\n                self.dict_map[cat_i] = index\n                index += 1\n        self.max_len = index + 1\n        \n    def transform(self, cat_list):\n        cat_transform_list = []\n        for cat_i in cat_list:\n            if cat_i in self.dict_map:\n                cat_transform_list.append(self.dict_map[cat_i])\n            else:\n                cat_transform_list.append(0)\n        return cat_transform_list\n#%% md\n# 浮点特征转换函数\n#%%\nclass float_deal:\n    def __init__(self):\n        self.max = 0\n        self.min = 0\n        self.max_min = 0 \n        \n    def fit(self, float_list):\n        for float_i in float_list:\n            if float_i < self.min:\n                self.min = float_i\n            if float_i > self.max:\n                self.max = float_i\n        self.max_min = self.max - self.min\n        \n    def transform(self, float_list):\n        float_transform_list = []\n        for float_i in float_list:\n            if float_i < self.min:\n                float_transform_list.append(0)\n            elif float_i > self.max:\n                float_transform_list.append(1)\n            else:\n                float_transform_list.append(float_i/self.max_min)\n        return float_transform_list\n    \n# 处理类别特征\ndict_cat_class = {}\nfor columns in ['user_id','content_id',\\\n                'task_container_id','prior_question_had_explanation',\\\n                'bundle_id','part']:\n    dict_cat_class[columns] = cat_deal()\n    dict_cat_class[columns].fit(train[columns])\n\n    train[columns] = dict_cat_class[columns].transform(train[columns])\n    print(columns)\n# str(content_id) + str(part) ---> embedding\n\n# 处理浮点特征\ndict_float_class = {}\nfor columns in ['timestamp','prior_question_elapsed_time']:\n    dict_float_class[columns] = float_deal()\n    dict_float_class[columns].fit(train[columns])\n    \n    train[columns] = dict_float_class[columns].transform(train[columns])\n    print(columns)","execution_count":7,"outputs":[{"output_type":"stream","text":"user_id\ncontent_id\ntask_container_id\nprior_question_had_explanation\nbundle_id\npart\ntimestamp\nprior_question_elapsed_time\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# 模型使用的函数（keras Lambda函数包装）\ndef squeeze(embedding):\n    embedding = tf.squeeze(embedding,axis=1)\n    return embedding\ndef concat(embedding_list):\n    embedding = tf.concat(embedding_list, axis=1)\n    return embedding\ndef multiply(multi_x_y):\n    multi_x = multi_x_y[0]\n    multi_y = multi_x_y[1]\n    multi_x_y = tf.multiply(multi_x, multi_y)\n    return multi_x_y","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 浮点数据输入\ninput_timestamp = tf.keras.Input(shape=(1,))\ninput_prior_question_elapsed_time = tf.keras.Input(shape=(1,))\n\n# 类别数据输入\ninput_user = tf.keras.Input(shape=(1,))\ninput_content = tf.keras.Input(shape=(1,))\ninput_task_container = tf.keras.Input(shape=(1,))\ninput_prior_question_had_explanation = tf.keras.Input(shape=(1,))\ninput_bundle = tf.keras.Input(shape=(1,))\ninput_part = tf.keras.Input(shape=(1,))\n\n# 所有输入\ninputs = [input_timestamp,input_prior_question_elapsed_time,\\\n         input_user,input_content,\\\n         input_task_container,input_prior_question_had_explanation,\\\n         input_bundle,input_part]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wide部分\n# 类别特征embeeding转换 \nembedding_user_wide = tf.keras.layers.Embedding(dict_cat_class['user_id'].max_len,\n                                           1, input_length=1)(input_user)\nembedding_user_wide = tf.keras.layers.Lambda(squeeze)(embedding_user_wide)\n\nembedding_content_wide = tf.keras.layers.Embedding(dict_cat_class['content_id'].max_len,\n                                              1, input_length=1)(input_content)\nembedding_content_wide = tf.keras.layers.Lambda(squeeze)(embedding_content_wide)\n\nembedding_task_container_wide = tf.keras.layers.Embedding(dict_cat_class['task_container_id'].max_len,\n                                                     1, input_length=1)(input_task_container)\nembedding_task_container_wide = tf.keras.layers.Lambda(squeeze)(embedding_task_container_wide)\n\nembedding_prior_question_had_explanation_wide = tf.keras.layers.Embedding(dict_cat_class['prior_question_had_explanation'].max_len, \n                                                                     1, input_length=1)(input_prior_question_had_explanation)\nembedding_prior_question_had_explanation_wide = tf.keras.layers.Lambda(squeeze)(embedding_prior_question_had_explanation_wide)\n\nembedding_bundle_wide = tf.keras.layers.Embedding(dict_cat_class['bundle_id'].max_len,\n                                             1, input_length=1)(input_bundle)\nembedding_bundle_wide = tf.keras.layers.Lambda(squeeze)(embedding_bundle_wide)\n\nembedding_part_wide = tf.keras.layers.Embedding(dict_cat_class['part'].max_len,\n                                           1, input_length=1)(input_part)\nembedding_part_wide = tf.keras.layers.Lambda(squeeze)(embedding_part_wide)\n\n# 合并类别特征对应的embeeding特征和浮点特征\nembedding_all = [input_timestamp,input_prior_question_elapsed_time,\\\n                embedding_user_wide, embedding_content_wide, embedding_task_container_wide,\\\n                embedding_prior_question_had_explanation_wide, embedding_bundle_wide, embedding_part_wide]\n\nwide_all = embedding_all + [input_timestamp,input_prior_question_elapsed_time]\n\n# wide layer\nwide_layer = tf.keras.layers.Lambda(concat)(wide_all) ","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deep部分\n# 类别特征embeeding转换 \nembedding_user_deep = tf.keras.layers.Embedding(dict_cat_class['user_id'].max_len,\n                                           8, input_length=1)(input_user)\nembedding_user_deep = tf.keras.layers.Lambda(squeeze)(embedding_user_deep)\n\nembedding_content_deep = tf.keras.layers.Embedding(dict_cat_class['content_id'].max_len,\n                                              8, input_length=1)(input_content)\nembedding_content_deep = tf.keras.layers.Lambda(squeeze)(embedding_content_deep)\n\nembedding_task_container_deep = tf.keras.layers.Embedding(dict_cat_class['task_container_id'].max_len,\n                                                     8, input_length=1)(input_task_container)\nembedding_task_container_deep = tf.keras.layers.Lambda(squeeze)(embedding_task_container_deep)\n\nembedding_prior_question_had_explanation_deep = tf.keras.layers.Embedding(dict_cat_class['prior_question_had_explanation'].max_len, \n                                                                     8, input_length=1)(input_prior_question_had_explanation)\nembedding_prior_question_had_explanation_deep = tf.keras.layers.Lambda(squeeze)(embedding_prior_question_had_explanation_deep)\n\nembedding_bundle_deep = tf.keras.layers.Embedding(dict_cat_class['bundle_id'].max_len,\n                                             8, input_length=1)(input_bundle)\nembedding_bundle_deep = tf.keras.layers.Lambda(squeeze)(embedding_bundle_deep)\n\nembedding_part_deep = tf.keras.layers.Embedding(dict_cat_class['part'].max_len,\n                                           8, input_length=1)(input_part)\nembedding_part_deep = tf.keras.layers.Lambda(squeeze)(embedding_part_deep)\n\n# 合并类别特征对应的embeeding特征和浮点特征\nembedding_all = [input_timestamp,input_prior_question_elapsed_time,\\\n                embedding_user_deep, embedding_content_deep, embedding_task_container_deep,\\\n                embedding_prior_question_had_explanation_deep, embedding_bundle_deep, embedding_part_deep]\n\ndeep_all = embedding_all \n\n# wide layer\ndeep_layer = tf.keras.layers.Lambda(concat)(deep_all) \n\n# fc \nfor unit_i in [16,256,16]:\n    deep_layer = tf.keras.layers.Dense(1, activation=tf.nn.relu)(deep_layer)\n\n# embedding_all 50 * 16 * 256 * 16\n\n\n# 合并 Wide部分 和 Deep部分 接fc\ndeep_wide_layer = tf.keras.layers.Lambda(concat)([deep_layer,wide_layer]) \n    \n# logit = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(deep_wide_layer)\nlogit = tf.expand_dims(tf.nn.sigmoid(tf.reduce_sum(deep_wide_layer,axis=1)),axis=1)\n\n# tf.nn.sigmoid(tf.reduce_sum(deep_wide_layer,axis=1))\n\n# 模型输入和输出\n\nmodel = tf.keras.models.Model(inputs=inputs, outputs=logit)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 编译模型\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam',\n              metrics=['binary_crossentropy'])\n\n# 设置学习率自动0.1减少函数\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                            verbose=1,\n                            mode='min',\n                            factor=0.1,\n                            patience=6)\n\n# 设置早停函数\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               verbose=1,\n                               mode='min',\n                               patience=10)\n\n# 设置保存点\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(f'fold.h5',\n                             monitor='val_loss',\n                             verbose=1,\n                             mode='min',\n                             save_best_only=True)\n\n# 查看模型结构\nmodel.summary()\n","execution_count":16,"outputs":[{"output_type":"stream","text":"Model: \"functional_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_5 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_7 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_8 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nembedding_12 (Embedding)        (None, 1, 8)         30608       input_3[0][0]                    \n__________________________________________________________________________________________________\nembedding_13 (Embedding)        (None, 1, 8)         95768       input_4[0][0]                    \n__________________________________________________________________________________________________\nembedding_14 (Embedding)        (None, 1, 8)         2064        input_5[0][0]                    \n__________________________________________________________________________________________________\nembedding_15 (Embedding)        (None, 1, 8)         40          input_6[0][0]                    \n__________________________________________________________________________________________________\nembedding_16 (Embedding)        (None, 1, 8)         71192       input_7[0][0]                    \n__________________________________________________________________________________________________\nembedding_17 (Embedding)        (None, 1, 8)         72          input_8[0][0]                    \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nlambda_15 (Lambda)              (None, 8)            0           embedding_12[0][0]               \n__________________________________________________________________________________________________\nlambda_16 (Lambda)              (None, 8)            0           embedding_13[0][0]               \n__________________________________________________________________________________________________\nlambda_17 (Lambda)              (None, 8)            0           embedding_14[0][0]               \n__________________________________________________________________________________________________\nlambda_18 (Lambda)              (None, 8)            0           embedding_15[0][0]               \n__________________________________________________________________________________________________\nlambda_19 (Lambda)              (None, 8)            0           embedding_16[0][0]               \n__________________________________________________________________________________________________\nlambda_20 (Lambda)              (None, 8)            0           embedding_17[0][0]               \n__________________________________________________________________________________________________\nlambda_21 (Lambda)              (None, 50)           0           input_1[0][0]                    \n                                                                 input_2[0][0]                    \n                                                                 lambda_15[0][0]                  \n                                                                 lambda_16[0][0]                  \n                                                                 lambda_17[0][0]                  \n                                                                 lambda_18[0][0]                  \n                                                                 lambda_19[0][0]                  \n                                                                 lambda_20[0][0]                  \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            51          lambda_21[0][0]                  \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 1, 1)         3826        input_3[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 1, 1)         11971       input_4[0][0]                    \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 1, 1)         258         input_5[0][0]                    \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 1, 1)         5           input_6[0][0]                    \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 1, 1)         8899        input_7[0][0]                    \n__________________________________________________________________________________________________\nembedding_5 (Embedding)         (None, 1, 1)         9           input_8[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 1)            2           dense_3[0][0]                    \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 1)            0           embedding[0][0]                  \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 1)            0           embedding_1[0][0]                \n__________________________________________________________________________________________________\nlambda_2 (Lambda)               (None, 1)            0           embedding_2[0][0]                \n__________________________________________________________________________________________________\nlambda_3 (Lambda)               (None, 1)            0           embedding_3[0][0]                \n__________________________________________________________________________________________________\nlambda_4 (Lambda)               (None, 1)            0           embedding_4[0][0]                \n__________________________________________________________________________________________________\nlambda_5 (Lambda)               (None, 1)            0           embedding_5[0][0]                \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 1)            2           dense_4[0][0]                    \n__________________________________________________________________________________________________\nlambda_6 (Lambda)               (None, 10)           0           input_1[0][0]                    \n                                                                 input_2[0][0]                    \n                                                                 lambda[0][0]                     \n                                                                 lambda_1[0][0]                   \n                                                                 lambda_2[0][0]                   \n                                                                 lambda_3[0][0]                   \n                                                                 lambda_4[0][0]                   \n                                                                 lambda_5[0][0]                   \n                                                                 input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nlambda_22 (Lambda)              (None, 11)           0           dense_5[0][0]                    \n                                                                 lambda_6[0][0]                   \n__________________________________________________________________________________________________\ntf_op_layer_Sum_1 (TensorFlowOp [(None,)]            0           lambda_22[0][0]                  \n__________________________________________________________________________________________________\ntf_op_layer_Sigmoid_1 (TensorFl [(None,)]            0           tf_op_layer_Sum_1[0][0]          \n__________________________________________________________________________________________________\ntf_op_layer_ExpandDims_1 (Tenso [(None, 1)]          0           tf_op_layer_Sigmoid_1[0][0]      \n==================================================================================================\nTotal params: 224,767\nTrainable params: 224,767\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 验证数据获取、剩下的数据为训练数据\nvalid = pd.DataFrame()\nfor i in range(6):\n    \n    # 获取验证标签数据\n    last_records = train.drop_duplicates('user_id', keep='last')\n    \n    # 获取验证标签以前的数据\n    map__last_records__user_row = dict(zip(last_records['user_id'],last_records['row_id']))\n    train['filter_row'] = train['user_id'].map(map__last_records__user_row)\n    train = train[train['row_id']<train['filter_row']]\n\n    # 特征加入验证集\n    valid = valid.append(last_records)\n    print(len(valid))\n\n# 数据特征切分以及删除train、valid，减少内存\n\n# 训练模型特征\nfeatures_columns = ['timestamp','prior_question_elapsed_time',\\\n                    'user_id','content_id',\\\n                    'task_container_id','prior_question_had_explanation',\\\n                    'bundle_id','part']\n\n# 训练和测试集数据切分\nX_valid, y_valid = [valid[columns].values for columns in features_columns], valid['answered_correctly'].values\n# del valid\nX_train, y_train = [train[columns].values for columns in features_columns], train['answered_correctly'].values\n# del train\n# 训练模型","execution_count":17,"outputs":[{"output_type":"stream","text":"3819\n7626\n11432\n15236\n19034\n22819\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=10,\n          batch_size=512 * 500 * 2,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_valid, y_valid),\n          callbacks=[plateau, early_stopping, checkpoint])\n\n# 测试集验证结果\ny_valid_proba = model.predict(X_valid, verbose=0, batch_size=512)\nauc = roc_auc_score(y_valid, y_valid_proba)\nprint(auc)","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n1/1 [==============================] - ETA: 0s - loss: 0.6715 - binary_crossentropy: 0.6715\nEpoch 00001: val_loss improved from inf to 0.67972, saving model to fold.h5\n1/1 [==============================] - 0s 366ms/step - loss: 0.6715 - binary_crossentropy: 0.6715 - val_loss: 0.6797 - val_binary_crossentropy: 0.6797\nEpoch 2/10\n1/1 [==============================] - ETA: 0s - loss: 0.6706 - binary_crossentropy: 0.6706\nEpoch 00002: val_loss improved from 0.67972 to 0.67911, saving model to fold.h5\n1/1 [==============================] - 0s 100ms/step - loss: 0.6706 - binary_crossentropy: 0.6706 - val_loss: 0.6791 - val_binary_crossentropy: 0.6791\nEpoch 3/10\n1/1 [==============================] - ETA: 0s - loss: 0.6696 - binary_crossentropy: 0.6696\nEpoch 00003: val_loss improved from 0.67911 to 0.67848, saving model to fold.h5\n1/1 [==============================] - 0s 104ms/step - loss: 0.6696 - binary_crossentropy: 0.6696 - val_loss: 0.6785 - val_binary_crossentropy: 0.6785\nEpoch 4/10\n1/1 [==============================] - ETA: 0s - loss: 0.6686 - binary_crossentropy: 0.6686\nEpoch 00004: val_loss improved from 0.67848 to 0.67786, saving model to fold.h5\n1/1 [==============================] - 0s 101ms/step - loss: 0.6686 - binary_crossentropy: 0.6686 - val_loss: 0.6779 - val_binary_crossentropy: 0.6779\nEpoch 5/10\n1/1 [==============================] - ETA: 0s - loss: 0.6677 - binary_crossentropy: 0.6677\nEpoch 00005: val_loss improved from 0.67786 to 0.67723, saving model to fold.h5\n1/1 [==============================] - 0s 99ms/step - loss: 0.6677 - binary_crossentropy: 0.6677 - val_loss: 0.6772 - val_binary_crossentropy: 0.6772\nEpoch 6/10\n1/1 [==============================] - ETA: 0s - loss: 0.6667 - binary_crossentropy: 0.6667\nEpoch 00006: val_loss improved from 0.67723 to 0.67661, saving model to fold.h5\n1/1 [==============================] - 0s 102ms/step - loss: 0.6667 - binary_crossentropy: 0.6667 - val_loss: 0.6766 - val_binary_crossentropy: 0.6766\nEpoch 7/10\n1/1 [==============================] - ETA: 0s - loss: 0.6657 - binary_crossentropy: 0.6657\nEpoch 00007: val_loss improved from 0.67661 to 0.67598, saving model to fold.h5\n1/1 [==============================] - 0s 100ms/step - loss: 0.6657 - binary_crossentropy: 0.6657 - val_loss: 0.6760 - val_binary_crossentropy: 0.6760\nEpoch 8/10\n1/1 [==============================] - ETA: 0s - loss: 0.6647 - binary_crossentropy: 0.6647\nEpoch 00008: val_loss improved from 0.67598 to 0.67535, saving model to fold.h5\n1/1 [==============================] - 0s 105ms/step - loss: 0.6647 - binary_crossentropy: 0.6647 - val_loss: 0.6754 - val_binary_crossentropy: 0.6754\nEpoch 9/10\n1/1 [==============================] - ETA: 0s - loss: 0.6636 - binary_crossentropy: 0.6636\nEpoch 00009: val_loss improved from 0.67535 to 0.67472, saving model to fold.h5\n1/1 [==============================] - 0s 102ms/step - loss: 0.6636 - binary_crossentropy: 0.6636 - val_loss: 0.6747 - val_binary_crossentropy: 0.6747\nEpoch 10/10\n1/1 [==============================] - ETA: 0s - loss: 0.6626 - binary_crossentropy: 0.6626\nEpoch 00010: val_loss improved from 0.67472 to 0.67410, saving model to fold.h5\n1/1 [==============================] - 0s 105ms/step - loss: 0.6626 - binary_crossentropy: 0.6626 - val_loss: 0.6741 - val_binary_crossentropy: 0.6741\n0.6295716256739535\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# 继续训练和验证\nmodel.fit(X_train, y_train,\n          epochs=1,\n          batch_size=512 * 500 * 2,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_valid, y_valid),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_valid_proba = model.predict(X_valid, verbose=0, batch_size=512)\nauc = roc_auc_score(y_valid, y_valid_proba)\nprint(auc)\n#%% md\n# 测试集环境\n#%%\n# iter_test = env.iter_test()\n#%%\n# for (test_df, sample_prediction_df) in iter_test:\n\n#     # 处理特征\n#     test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].map({'True':1,'False':0}).fillna(-1).astype(np.int8)\n\n#     # 合并样本\n#     test_df = pd.merge(\n#         left=test_df,\n#         right=questions,\n#         how='left',\n#         left_on='content_id',\n#         right_on='question_id'\n#         )\n\n#     test_df = test_df.fillna(0)\n\n\n#     for columns in ['user_id','content_id',\\\n#                     'task_container_id','prior_question_had_explanation',\\\n#                     'bundle_id','part']:\n\n#         test_df[columns] = dict_cat_class[columns].transform(test_df[columns])\n#         print(columns)\n\n\n#     for columns in ['timestamp','prior_question_elapsed_time']:\n      \n#         test_df[columns] = dict_float_class[columns].transform(test_df[columns])\n#         print(columns)\n\n#     X_test = [test_df[columns].values for columns in features_columns]\n\n#     test_df['answered_correctly'] =  model.predict(X_test, verbose=0, batch_size=512)\n#     env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n\n","execution_count":19,"outputs":[{"output_type":"stream","text":"1/1 [==============================] - ETA: 0s - loss: 0.6616 - binary_crossentropy: 0.6616\nEpoch 00001: val_loss improved from 0.67410 to 0.67348, saving model to fold.h5\n1/1 [==============================] - 0s 155ms/step - loss: 0.6616 - binary_crossentropy: 0.6616 - val_loss: 0.6735 - val_binary_crossentropy: 0.6735\n0.6319446535373028\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}